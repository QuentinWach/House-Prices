{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 12477.283999999985\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from sklearn.linear_model import LinearRegression,Ridge, Lasso\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Separate target from predictors\n",
    "data=pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1)\n",
    "#df = test.drop(columns = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'])\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.001,\n",
    "                                                                random_state=42)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and \n",
    "                       X_train[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_traina = X_train[my_cols].copy()\n",
    "X_testa = X_test[my_cols].copy()\n",
    "\n",
    "#Similar to how a pipeline bundles together preprocessing and modeling steps,\n",
    "#we use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n",
    "\n",
    "#imputes missing values in numerical data, and\n",
    "#imputes missing values and applies a one-hot encoding to categorical data.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=3)\n",
    "#Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:\n",
    "\n",
    "#With the pipeline, we preprocess the training data and fit the model in a single line of code. \n",
    "#(In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. \n",
    "#This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "#With the pipeline, we supply the unprocessed features in X_valid to the predict() command, \n",
    "#and the pipeline automatically preprocesses the features before generating predictions. \n",
    "#(However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#TestId=df['Id']\n",
    "#align data set shapes and get dummies\n",
    "#total_features=pd.concat((data.drop(['Id','SalePrice'], axis=1), df.drop(['Id'], axis=1)))\n",
    "#total_features=pd.get_dummies(total_features, drop_first=True)\n",
    "#train_features=test[0:data.shape[0]]\n",
    "\n",
    "#making sure the test set matches the train set\n",
    "test_features=test\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "preds\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_test, preds)\n",
    "print('MAE:', score)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "preds=my_pipeline.predict(test_features)\n",
    "output = pd.DataFrame({'Id': test.Id,\n",
    "                     'SalePrice': preds})\n",
    "output.to_csv('submission1.csv', index=False)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_description.txt  sample_submission.csv  test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls '../input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test=pd.read_csv('../input/test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
