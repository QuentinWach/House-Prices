{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7238ae8d-73dc-474b-9d36-92647cdc6030",
    "_execution_state": "idle",
    "_uuid": "691c624259cf2737f7e29c2fdbbc7f255e842eb2"
   },
   "source": [
    "![png](./docs/header.jpg)\n",
    "\n",
    "# House Prices -  `Model V8`\n",
    "### Top 9% on Kaggle August 27th, 2019 \n",
    "\n",
    "This approach combines a lot of rather rough **feature engeneering**:\n",
    "+ [X] imputing missing values\n",
    "+ [X] deleting obvious outliers\n",
    "+ [X] encoding categorical variables\n",
    "+ [X] creatin dummy features\n",
    "+ [X] transforming skewed features\n",
    "+ [X] adding new features\n",
    "\n",
    "and **stacking different regression models** with `SkLearn`, `XGBoost` and `LightGBM` simply taking the mean of their predictions as the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "This is the 8th iteration of my house price model. Of course I did a lot of data exploration aswell. This can be found in the other notebook in this repository. Through the process I got lots and lots of helpfull information and tips from all kinds of blogs and notebooks (linked in docs folder) most notably:\n",
    "\n",
    "+ https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n",
    "+ https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2dbccbd6-138b-4f1b-9b23-fd60c7525c14",
    "_execution_state": "idle",
    "_uuid": "c9b1d5dff21d39260eb47af6fe7aac4bd03be233"
   },
   "outputs": [],
   "source": [
    "# import some modules (later more)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "59617b4b-d797-44ce-9142-05fbfd36aada",
    "_execution_state": "idle",
    "_uuid": "0e694d13459e3e200f6e2c6333c887cbad779ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several features (nearly up to 40) that seem uncorrolated with the `SalePrice` nonetheless they become very helpfull to squeeze out the little\n",
    "rest to reach a top score! Yet the `Id` is irrelevant and will be droppped. So it `Utilities` as it adds no information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b24451a1-fb8c-4094-ad0b-0940469d07fc",
    "_execution_state": "idle",
    "_uuid": "687813c270cbfdedccc7a9e4ec9fbb78a99d54ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n",
      "(1459, 78)\n"
     ]
    }
   ],
   "source": [
    "# dropping features\n",
    "test_ID = test['Id']\n",
    "dropF = [\"Id\", \"Utilities\"]\n",
    "for f in dropF:\n",
    "    train = train.drop(f, axis = 1)\n",
    "    test = test.drop(f, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "465043f2-d687-4b1f-a6b4-1036859dfeb0",
    "_execution_state": "idle",
    "_uuid": "32b12bca723c5e867f7d7a7e179ff934a5fcdf30"
   },
   "source": [
    "I read in several notebooks about outlier detection. But I can't push enough how **dangerous** automating outlier detection is! Although it seemed to bring some improvement for models with only a small subset of the given features it worsened the it drastically when including all the lesser correlated features. I will only drop some obvious outliers **manually**! (See ....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "6c5780b2-d4a8-42d9-b902-c6a23eef7d99",
    "_execution_state": "idle",
    "_uuid": "583bb417102d7bebb4aaf14bcb1aebcae86443bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 79)\n",
      "(1459, 78)\n"
     ]
    }
   ],
   "source": [
    "# deleting outliers from GrLivArea\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "658f5b56-5830-486c-81a0-8514fb95e274",
    "_execution_state": "idle",
    "_uuid": "4b96a6a35983d1c765c11c929bcd32effd105b43"
   },
   "source": [
    "Although XBBoost alone generally doesn't need any feature normalization (and it even worsened smaller models) when using a stacked model transforming the SalePrice skew helped. Don't transform other features as there seem to be differences to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "21b3a0ad-bd68-49aa-a3d7-40a30b3c59dc",
    "_execution_state": "idle",
    "_uuid": "719cf6a9dca56cc529e97af21816d291fa8bd8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 79)\n",
      "(1459, 78)\n"
     ]
    }
   ],
   "source": [
    "# correct skew\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "1bd3e9b9-2f42-4251-aadd-5ced84eb1a27",
    "_execution_state": "idle",
    "_uuid": "efc576211e4eed962f04cd94d901c667e6912528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 79)\n",
      "(2917, 78)\n"
     ]
    }
   ],
   "source": [
    "# merge train and test into a single data frame\n",
    "y_train = train['SalePrice'].values\n",
    "data = pd.concat((train, test)).reset_index(drop=True)\n",
    "data = data.drop(['SalePrice'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "501b465f-8c80-4b93-81d0-a5d41e08d235",
    "_execution_state": "idle",
    "_uuid": "f97d25548ec8f6c02e2d1ee5a6df6c3d107fdf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 78)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>PoolQC</td>\n",
       "      <td>99.691464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MiscFeature</td>\n",
       "      <td>96.400411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Alley</td>\n",
       "      <td>93.212204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fence</td>\n",
       "      <td>80.425094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FireplaceQu</td>\n",
       "      <td>48.680151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Missing [%]\n",
       "PoolQC         99.691464\n",
       "MiscFeature    96.400411\n",
       "Alley          93.212204\n",
       "Fence          80.425094\n",
       "FireplaceQu    48.680151"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NaN values from the set\n",
    "dataNa = (data.isnull().sum() / len(data)) * 100\n",
    "dataNa = dataNa.drop(dataNa[dataNa == 0].index).sort_values(ascending=False)[:30]\n",
    "# how much data is missing now?\n",
    "missing = pd.DataFrame({'Missing [%]':dataNa})\n",
    "missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ca8a04eb-f42b-4c26-a690-bb98c95c6118",
    "_execution_state": "idle",
    "_uuid": "1d94b062f7683d711d479e48530009040185fd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 78)\n"
     ]
    }
   ],
   "source": [
    "# Filling nonexisting data\n",
    "nonFeature = [\"PoolQC\", \"Alley\", \"Fence\", \"MiscFeature\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\",\n",
    "              \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"MSSubClass\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \n",
    "              \"BsmtFinType1\", \"BsmtFinType2\", \"MasVnrType\"]\n",
    "for f in nonFeature:\n",
    "    data[f] = data[f].fillna(\"None\")\n",
    "\n",
    "# Filling missing data with zeros\n",
    "zeroFeature = [\"MasVnrArea\", \"GarageYrBlt\", \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \"BsmtFinSF2\",\n",
    "               \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\n",
    "for f in zeroFeature:\n",
    "    data[f] = data[f].fillna(0)\n",
    "\n",
    "# Filling missing data with the median\n",
    "data[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Filling categorical NaN features\n",
    "data[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\n",
    "catFeature = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\n",
    "for f in catFeature:\n",
    "    data[f] = data[f].fillna(data[f].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0adf05cf-ce60-4169-805c-ca776e60e85a",
    "_execution_state": "idle",
    "_uuid": "b091fa2ebef19425019e2e550410d0376b9e9fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 78)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing [%]]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NaN values from the set\n",
    "dataNa = (data.isnull().sum() / len(data)) * 100\n",
    "dataNa = dataNa.drop(dataNa[dataNa == 0].index).sort_values(ascending=False)[:30]\n",
    "# how much data is missing now?\n",
    "missing = pd.DataFrame({'Missing [%]':dataNa})\n",
    "missing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78266762-5180-44fa-a630-b808706800d4",
    "_execution_state": "idle",
    "_uuid": "360f518886ac45afe2963b9b53edb17c2be4a130"
   },
   "source": [
    "(Here we check if anything is still missing.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f80c0e7-3f3f-45c5-b111-e36f4e31e814",
    "_execution_state": "idle",
    "_uuid": "c4743ffb7fbb050edca7c77dc7cb6520577c1398"
   },
   "source": [
    "We will now encode the labels to categorical features using `SkLearn`. Might not be the best decision, but this way I avoided creating to much dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "81c97efb-4f76-4e87-861a-10a60ab5c84b",
    "_execution_state": "idle",
    "_uuid": "fdb5ddf0a49a3c6df303c569c9f3509c79ac8b61"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "catFeature = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "for f in catFeature:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(data[f].values)) \n",
    "    data[f] = lbl.transform(list(data[f].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4879ef7-ab0d-4955-bc48-7ebcfa04b3bd",
    "_execution_state": "idle",
    "_uuid": "9976d6288bc183d443fbccc2bde439d5bc3a87b1"
   },
   "source": [
    "Now the fun part! Let's add new features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "fc1a8f1a-f003-4538-8e60-d819f46362a3",
    "_execution_state": "idle",
    "_uuid": "208f8d22188786227fff4a978dc3b11b4e1ffd90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 79)\n",
      "(2917, 80)\n"
     ]
    }
   ],
   "source": [
    "# total squarefootage feature \n",
    "data[\"TotalSF\"] = data[\"TotalBsmtSF\"] + data[\"1stFlrSF\"] + data[\"2ndFlrSF\"]\n",
    "# people\n",
    "data[\"People\"] = 2 * data[\"GarageCars\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91c73aad-82d1-4301-b540-b2f69dc13902",
    "_execution_state": "idle",
    "_uuid": "aa36d6e3253e354b46d9c9c6f2e8a4089c76be16"
   },
   "source": [
    "And let's remove the skew from a lof of numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "c5972a73-7e86-4164-a9d6-58432dae1933",
    "_execution_state": "idle",
    "_uuid": "53c471c7008c66590f257e70866f8a3037813f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>MiscVal</td>\n",
       "      <td>21.939672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PoolArea</td>\n",
       "      <td>17.688664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LotArea</td>\n",
       "      <td>13.109495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LowQualFinSF</td>\n",
       "      <td>12.084539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3SsnPorch</td>\n",
       "      <td>11.372080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Skew\n",
       "MiscVal       21.939672\n",
       "PoolArea      17.688664\n",
       "LotArea       13.109495\n",
       "LowQualFinSF  12.084539\n",
       "3SsnPorch     11.372080"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numFeatures = data.dtypes[data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewedFeatures = data[numFeatures].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewed = pd.DataFrame({'Skew': skewedFeatures})\n",
    "skewed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f110087-b707-4073-a1df-0a0a9d6ccbd3",
    "_execution_state": "idle",
    "_uuid": "cf63bdc9f4f80d81f1bfa14f89d65ff104d45e5b"
   },
   "source": [
    "Using the Box Cox Transformation we minimize the skew of numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "d8ebce87-c55d-46c6-8f06-8b34116d7370",
    "_execution_state": "idle",
    "_uuid": "969fdff338ef46f064d8f855782c96d322a264b1"
   },
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "\n",
    "# list of highly skewed features\n",
    "skewed = skewed[abs(skewed) > 1]\n",
    "skewedFeatures = skewed.index\n",
    "\n",
    "# transform these features\n",
    "lamda = 0.2\n",
    "for f in skewedFeatures:\n",
    "    data[f] = boxcox1p(data[f], lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39639caf-31a4-4401-a663-0ba9536b39bf",
    "_execution_state": "idle",
    "_uuid": "5a13a6e2a3e48975de9129d1593bd38df44a1069"
   },
   "source": [
    "And we split our categorical features into several dummy features (0,1) increasing the number of features immensly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "c8e63516-e4e2-4f36-a60e-1c8316392c60",
    "_execution_state": "idle",
    "_uuid": "acd44e283867425257ffd1fb2f4893cdbff43f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917, 221)\n",
      "(2917, 221)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data = pd.get_dummies(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "243cf047-c2ba-4ae5-a531-22ef9b7cfbfe",
    "_execution_state": "idle",
    "_uuid": "fe9d78c7e37142ee8089826eca3065e0fa5803c1"
   },
   "source": [
    "Now, before we go into moddeling the processed data, let us split it again into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "0a75646f-1974-40ad-a085-ff7bc08454a5",
    "_execution_state": "idle",
    "_uuid": "89e464095544a53177d5a009b914ba4c660072a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458, 221)\n",
      "(1459, 221)\n"
     ]
    }
   ],
   "source": [
    "train = data[:ntrain]\n",
    "test = data[ntrain:]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "461af83d-a928-4645-8512-5e4dbcaf7be0",
    "_execution_state": "idle",
    "_uuid": "10aab4cee97832560e2627a490e01e80c0ffb814"
   },
   "source": [
    "---\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "135e8ac5-ce46-4a5f-b205-13f827ef33b8",
    "_execution_state": "idle",
    "_uuid": "fc664fbe27561a3697d0210921107b0e14b7d211"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "03f45cb7-0a40-45ea-94e8-64fd7ff1e8f6",
    "_execution_state": "idle",
    "_uuid": "2a50c954cb771d350c3092c3658486ba4d22aba5"
   },
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
    "# Elastic Net\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "# Kernel Ridge\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "# GB Regressor\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "# XGB Regressor\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "# Light GBM Regressor\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make individual predictions\n",
    "pred1 = lasso.fit(train,y_train).predict(test)\n",
    "pred2 = ENet.fit(train,y_train).predict(test)\n",
    "pred3 = KRR.fit(train,y_train).predict(test)\n",
    "pred4 = GBoost.fit(train,y_train).predict(test)\n",
    "pred5 = model_xgb.fit(train,y_train).predict(test)\n",
    "pred6 = model_lgb.fit(train,y_train).predict(test)\n",
    "\n",
    "# use the mean to make final predictions\n",
    "pred = []\n",
    "for e in range(len(pred1)):\n",
    "    pred.append(np.expm1(np.mean([pred1[e],pred2[e],pred3[e],pred4[e],pred5[e],pred6[e]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions for submission\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = pred\n",
    "sub.to_csv('submissionV8.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
